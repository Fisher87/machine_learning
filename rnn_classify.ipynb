{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import codecs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. #### 生成字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"./data/cnews/\")\n",
    "\n",
    "words = defaultdict(int)\n",
    "\n",
    "base_dir = 'data/cnews'\n",
    "train_dir = os.path.join(base_dir, 'cnews.train.txt')\n",
    "test_dir = os.path.join(base_dir, 'cnews.test.txt')\n",
    "val_dir = os.path.join(base_dir, 'cnews.val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "\n",
    "save_dir = 'checkpoints/textrnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')  # 最佳验证结果保存路径\n",
    "\n",
    "for f in files:\n",
    "    if not f.endswith(\".txt\"):continue\n",
    "    file_path = os.path.join(\"./data/cnews\", f)\n",
    "    with codecs.open(file_path, 'r', 'utf8') as rf:\n",
    "        for line in rf:\n",
    "            line = line.strip()\n",
    "            for w in line:\n",
    "                words[w] += 1\n",
    "\n",
    "sorted_words = sorted(words.items(), key=lambda x:x[1], reverse=True)\n",
    "newlines = list()\n",
    "for w, c in sorted_words:\n",
    "    newline = \"\\t\".join([w, str(c)])\n",
    "    newlines.append(newline)\n",
    "    \n",
    "with codecs.open(\"vocab.txt\", 'w', 'utf8') as wf:\n",
    "    wf.write('\\n'.join(newlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {\"UNK\":0}\n",
    "index2word = {0:\"UNK\"}\n",
    "with codecs.open('./vocab.txt', 'r', 'utf8') as rf:\n",
    "    for i, line in enumerate(rf):\n",
    "        if i >= 4999:break\n",
    "        word = line.strip().split('\\t')[0]\n",
    "        word2index[word] = i+1\n",
    "        index2word[i+1] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index(query):\n",
    "    query_index = [word2index.get(w, 0) for w in query]\n",
    "    return query_index\n",
    "\n",
    "def generate_word(indexs):\n",
    "    index_query = [index2word.get(i, \"UNK\") for i in indexs]\n",
    "    return index_query\n",
    "\n",
    "def read_category():\n",
    "    \"\"\"读取分类目录，固定\"\"\"\n",
    "    categories = [u'体育', u'财经', u'房产', u'家居', u'教育', u'科技', u'时尚', u'时政', u'游戏', u'娱乐']\n",
    "    categories = [x for x in categories]\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "    return categories, cat_to_id\n",
    "\n",
    "def read_file(filename):\n",
    "    contents, labels = list(), list()\n",
    "    with codecs.open(filename, 'r','utf8') as rf:\n",
    "        for line in rf:\n",
    "            try:\n",
    "                l = line.strip().split('\\t')\n",
    "#                 if len(l) != 2:\n",
    "#                     print line.strip()\n",
    "                label, content = l\n",
    "                if content:\n",
    "                    contents.append(list(content))\n",
    "                    labels.append(label)\n",
    "            except Exception as e:\n",
    "                continue              \n",
    "    return contents, labels                \n",
    "\n",
    "# contents, labels = read_file(\"./data/cnews/cnews.train.txt\")\n",
    "# print contents[0], labels[0]\n",
    "categories, cat2id = read_category()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成训练集和预测集\n",
    "\n",
    "def generate_data(filename, word2id, cat2id, maxlen=500):\n",
    "    contents, labels = read_file(filename)\n",
    "    \n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word2index.get(w, 0) for w in contents[i]])\n",
    "        label_id.append(cat2id.get(labels[i]))\n",
    "        \n",
    "    x_pad = tf.keras.preprocessing.sequence.pad_sequences(data_id, maxlen)\n",
    "    # change to one_hot coding\n",
    "    y_pad = tf.keras.utils.to_categorical(label_id, num_classes=len(cat2id))\n",
    "    # y_pad = tf.one_hot(label_id, len(cat2id))\n",
    "    return x_pad, y_pad\n",
    "\n",
    "def get_embedding(vocab_size, embedding_dim, input_x):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding_table = tf.get_variable('embedding', [vocab_size, embedding_dim], reuse=True)\n",
    "        embedding = tf.nn.embedding_lookup(embedding_table, input_x)\n",
    "        return embedding\n",
    "    \n",
    "def get_batch(x, y, batch_size=64):\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len-1) / batch_size ) + 1\n",
    "    \n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    "    \n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i+1)*batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n",
    "        \n",
    "def feed_data(model, x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "         model.input_x: x_batch,\n",
    "         model.input_y: y_batch,\n",
    "         model.keep_prob: keep_prob\n",
    "     }\n",
    "    return feed_dict\n",
    "\n",
    "def evaluate(sess, x_, y_, model):\n",
    "    data_len = len(x_)\n",
    "    batch_eval = get_batch(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(model, x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config配置\n",
    "class Config(object):\n",
    "    # 模型参数\n",
    "    embedding_dim = 64      # 词向量维度\n",
    "    seq_length = 600        # 序列长度\n",
    "    num_classes = 10        # 类别数\n",
    "    vocab_size = 5000       # 词汇表达小\n",
    "\n",
    "    num_layers= 2           # 隐藏层层数\n",
    "    hidden_dim = 128        # 隐藏层神经元\n",
    "    rnn = 'gru'             # lstm 或 gru\n",
    "\n",
    "    dropout_keep_prob = 0.8 # dropout保留比例\n",
    "    learning_rate = 1e-3    # 学习率\n",
    "\n",
    "    batch_size = 128         # 每批训练大小\n",
    "    num_epochs = 10          # 总迭代轮次\n",
    "\n",
    "    print_per_batch = 100    # 每多少轮输出一次结果\n",
    "    save_per_batch = 10      # 每多少轮存入tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextRNN模型\n",
    "class TextRNN(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.input_x = tf.placeholder(tf.int32, shape=[None, self.config.seq_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, shape=[None, self.config.num_classes], name=\"input_y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        \n",
    "        self.rnn()\n",
    "        \n",
    "    def rnn(self):\n",
    "        \n",
    "        with tf.variable_scope(\"rnn\", reuse=tf.AUTO_REUSE):\n",
    "            def lstm_cell():\n",
    "                return tf.contrib.rnn.BasicLSTMCell(self.config.hidden_dim, state_is_tuple=True)\n",
    "            def gru_cell():\n",
    "                return tf.contrib.rnn.GRUCell(self.config.hidden_dim)\n",
    "            def dropout():\n",
    "                if self.config.rnn==\"lstm\":\n",
    "                    cell = lstm_cell()\n",
    "                else:\n",
    "                    cell = gru_cell()\n",
    "                return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.config.dropout_keep_prob)\n",
    "        \n",
    "#         embedding_inputs = get_embedding(self.config.vocab_size, self.config.embedding_dim, self.input_x)\n",
    "            with tf.device('/cpu:0'):\n",
    "            \n",
    "                embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim])\n",
    "                embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "        \n",
    "            with tf.name_scope(\"rnn\"):\n",
    "            \n",
    "            # 多层rnn\n",
    "                cells = [dropout() for _ in range(self.config.num_layers)]\n",
    "                rnn_cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "            \n",
    "                _outputs, _ = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=embedding_inputs, dtype=tf.float32)\n",
    "                last = _outputs[:, -1, :]   #取最后一个时序输出\n",
    "            \n",
    "            with tf.name_scope(\"score\"):\n",
    "            # 全连接层，后面接dropout以及Relu\n",
    "                fc = tf.layers.dense(last, self.config.hidden_dim, name=\"fc1\")\n",
    "                fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "                fc = tf.nn.relu(fc)\n",
    "            \n",
    "            # 分类器\n",
    "                self.logits = tf.layers.dense(fc, self.config.num_classes, name=\"fc2\")\n",
    "                self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)\n",
    "            \n",
    "            with tf.name_scope('optimize'):\n",
    "            # 损失函数，交叉熵\n",
    "                cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "                self.loss = tf.reduce_mean(cross_entropy)\n",
    "            \n",
    "                self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "                \n",
    "            with tf.name_scope(\"accuracy\"):\n",
    "            # 准确率\n",
    "                correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "                self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TRNNConfig(object):\n",
    "    embedding_dim = 64\n",
    "    seq_length = 600\n",
    "    num_classes = 10\n",
    "    vocab_size = 5000\n",
    "    \n",
    "    num_layers = 2\n",
    "    hidden_dim = 128\n",
    "    rnn = 'gru'\n",
    "    \n",
    "    dropout_keep_prob = 0.8\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    batch_size = 128\n",
    "    num_epochs = 10\n",
    "    \n",
    "    print_per_batch = 100\n",
    "    save_per_batch = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train module\n",
    "def train():\n",
    "    config = TRNNConfig()\n",
    "    model = TextRNN(config)\n",
    "    print \"config tensorboard and saver\"\n",
    "    tensorboard_dir = \"tensorboard/textrnn\"\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    # train start\n",
    "    x_train, y_train = generate_data(train_dir, word2index, cat2id, config.seq_length)\n",
    "    x_val, y_val = generate_data(val_dir, word2index, cat2id, config.seq_length)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # 总批次\n",
    "    best_acc_val = 0.0  # 最佳验证集准确率\n",
    "    last_improved = 0  # 记录上一次提升批次\n",
    "    require_improvement = 1000  # 如果超过1000轮未提升，提前结束训练\n",
    "            \n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = get_batch(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "#             print \"xxxxx x_batch shape: \", x_batch.shape\n",
    "#             print \"xxxxx y_batch shape: \", y_batch.shape\n",
    "#             print y_batch\n",
    "#             print config.dropout_keep_prob\n",
    "            feed_dict = feed_data(model, x_batch, y_batch, config.dropout_keep_prob)\n",
    "\n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                 # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                s = sess.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                 # 每多少轮次输出在训练集和验证集上的性能\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(sess, x_val, y_val, model)  # todo\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                     # 保存最好结果\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=sess, save_path=save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                       + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            feed_dict[model.keep_prob] = config.dropout_keep_prob\n",
    "            sess.run(model.optim, feed_dict=feed_dict)  # 运行优化\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 验证集正确率长期不提升，提前结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "        if flag:  # 同上\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config tensorboard and saver\n",
      "Training and evaluating...\n",
      "('Epoch:', 1)\n",
      "Iter:      0, Train Loss:    2.3, Train Acc:  12.50%, Val Loss:    2.3, Val Acc:   9.14%, Time: 0:00:25 *\n",
      "Iter:    100, Train Loss:    1.0, Train Acc:  73.44%, Val Loss:    1.2, Val Acc:  57.76%, Time: 0:03:58 *\n",
      "Iter:    200, Train Loss:   0.48, Train Acc:  82.81%, Val Loss:   0.76, Val Acc:  74.78%, Time: 0:07:31 *\n",
      "Iter:    300, Train Loss:   0.46, Train Acc:  85.94%, Val Loss:   0.61, Val Acc:  81.90%, Time: 0:11:12 *\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-f23c21c7cab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-6b216dd38226>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 运行优化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mtotal_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
