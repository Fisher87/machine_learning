{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 决策树是一种预测模型；决策树仅有单一输出，如果有多个输出，可以分别建立独立的决策树以处理不同的输出。如图所示：\n",
    "\n",
    "\n",
    "![](http://img.blog.csdn.net/20161121193225546)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.信息熵\n",
    "信息熵(`information entropy`)定义为离散随机事件出现的概率，一个系统**越是稳定**，信息熵就**越低**，反之一个系统**越是混乱**，则它的信息熵**越高**；所以信息熵可以被认为是系统稳定性的一个度量。\n",
    "\n",
    "*如何计算信息熵？*\n",
    "\n",
    "对于有`K`个 类别的分类问题来说，假定样本集合`D`中第`k`类样本所占的比例为`p_k(k=1,2,...K)`， 则样本集合`D`的信息熵为:\n",
    "![image.png](./docs/xinxishang.png)\n",
    "\n",
    "### 2.信息增益\n",
    "信息增益是针对一个特定的分支标准，在该分支下计算原有数据的信息熵与引入该分支标准后的信息熵之差；\n",
    "\n",
    "**定义**\n",
    "\n",
    "![image.png](./docs/xinxizengyi.png)\n",
    "\n",
    "例如：\n",
    "![](./docs/xinxizengyi_lizi.png)\n",
    "\n",
    "通过表1中天气的4个特征(`outlook, temperature, humidity, windy`)来分类目标(`play, not play`)\n",
    "\n",
    "![image.png](./docs/xixinzengyi_jie.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树构造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3算法\n",
    "> ID3算法是一种贪心算法，用来构造决策树。ID3算法中根据特征选择和信息增益评估分支标准，每次选择**信息增益最大**的特征作为分支标准。\n",
    "\n",
    "#### 算法思想\n",
    "ID3算法的基本思想是：首先计算出原始数据集的信息熵，然后依次将数据中的每一个特征作为分支标准，并计算其相对于原始数据的信息增益，选择最大信息增益的分支标准来划分数据，因为信息增益越大，区分样本的能力就越强，越具有代表性。重复上述过程从而生成一棵决策树，很显然这是一种自顶向下的贪心策略。\n",
    "#### 算法缺陷\n",
    "+ 使用信息增益有一个缺点，那就是它会偏向于具有大量值的属性，也就是说在训练集中某个属性所取得不同值的个数越多，则越有可能选择它来作为分类属性，而这么做有时候是没有意义的；\n",
    "+ ID3算法不能处理连续分布的数据特征；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.5算法\n",
    "> C4.5算法是对ID3算法的改进，它克服了ID3的**两个**缺点；对于离散的特征，C4.5算法**不直接使用**信息增益，而是使用**增益率(`gain ratio`)**来选择最优的分支标准；\n",
    "\n",
    "#### 增益率\n",
    "定义：\n",
    "\n",
    "![image.png](./docs/zengyilv.png?raw=true)\n",
    "\n",
    "#### 连续值属性处理方法\n",
    "对于连续属性值，C4.5算法处理的方法是**先把连续属性值转换为离散属性在进行处理；**虽然本质上属性的取值是连续的，但对于有限的采样数据它是离散的，如果有N条样本，那么我们有N-1种离散化的方法：<=vj的分到左子树，>vj的分到右子树。计算这N-1种情况下最大的信息增益率。在离散属性上只需要计算1次信息增益率，而在连续属性上却需要计算N-1次，计算量是相当大的。通过以下办法可以减少计算量：对于连续属性先按大小进行排序，只有在分类发生改变的地方才需要切开。例如：\n",
    "![image.png](./docs/C4.5.png?raw=true)\n",
    "\n",
    "对表中的连续属性Temperature进行分析，首先对样本根据Temperature属性值进行排序，结果如上表所示。在分类改变的地方(表中红线部分)进行切开，本来有13种离散化的情况，现在只需计算7种。\n",
    "\n",
    "如果利用增益率来选择连续值属性的分界点，会导致一些副作用。分界点将样本分成两个部分，这两个部分的样本个数之比也会影响增益率。根据增益率公式，我们可以发现，当分界点能够把样本分成数量相等的两个子集时（我们称此时的分界点为等分分界点），增益率的抑制会被最大化，因此等分分界点被过分抑制了。子集样本个数能够影响分界点，显然不合理。因此在决定分界点是还是采用增益这个指标，而选择属性的时候才使用增益率这个指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART算法\n",
    "> 上面介绍的ID3，C4.5算法都是基于信息熵来进行选取划分节点的，主要用于**分类问题**，而CART决策树全称为分类回归树(Classification And Regression Tree)，分类回归问题都可以使用\n",
    "\n",
    "CART和前面两种算法不同的地方是，在每一次节点做判断时，只考虑二分类的情况，即使特征能够取到多个值（比如属性颜色有红、黄、蓝三种取值，ID3和C4.5直接就划分为红、黄、蓝三个子类，而CART只能在一次划分时划分为是不是红（黄、蓝）然后再进行判断。\n",
    "\n",
    "#### 基尼指数:$$ Gini(D)=1- \\sum_{k=1}^N p_{k}^2$$\n",
    "其中$p_{k}$和前文相同，表示第k类样本所占的比例，基尼指数同样可以视为衡量选取划分特征有效程度的一个指标。当样本非常不均匀的时候，基尼指数较小，当样本均匀的时候，基尼指数较大。**因此，我们的目标是：选取划分后基尼指数最小的属性（最能够区分样本的属性）进行划分。**\n",
    "\n",
    "#### 定义属性a的基尼指数:$$ Gini_{a} = \\sum_{v=1}^V \\frac{|D^v|}{|D|}Gini(D^v)$$\n",
    "因此,我们选择$Gini_{a}$ 最小的特征进行划分\n",
    "\n",
    "[例子](https://zhuanlan.zhihu.com/p/32003259)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outlook': {0: 'N', 1: 'Y', 2: {'windy': {0: 'Y', 1: 'N'}}}}\n"
     ]
    }
   ],
   "source": [
    "# _*_ coding:utf8 _*_\n",
    "\n",
    "\"\"\"\n",
    "C4.5构建DTree\n",
    "\"\"\"\n",
    "from math import log\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建数据\n",
    "def createDataSet():\n",
    "    dataSet = [[0, 0, 0, 0, 'N'],\n",
    "               [0, 0, 0, 1, 'N'],\n",
    "               [1, 0, 0, 0, 'Y'],\n",
    "               [2, 1, 0, 0, 'Y'],\n",
    "               [2, 2, 1, 0, 'Y'],\n",
    "               [2, 2, 1, 1, 'N'],\n",
    "               [1, 2, 1, 1, 'Y']]\n",
    "    labels = ['outlook', 'temperature', 'humidity', 'windy']\n",
    "    return dataSet, labels\n",
    "\n",
    "# 计算熵\n",
    "def calcEnt(dataset):\n",
    "    numEntries = len(dataset)\n",
    "    labelCounts = {}\n",
    "    for featvec in dataset:\n",
    "        currentLabel = featvec[-1]\n",
    "        if currentLabel not in labelCounts:\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1\n",
    "\n",
    "    ent = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key])/numEntries\n",
    "        ent -= prob * log(prob,2)\n",
    "    return ent\n",
    "\n",
    "# 采用多数判决的方法决定子节点的分类\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount:\n",
    "            classCount[vote]=0\n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reversed=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "# 选择最大的gain ratio对应的feature\n",
    "def chooseBestFeature(dataset):\n",
    "    numFeatures = len(dataset[0]) - 1\n",
    "    baseEnt = calcEnt(dataset)        # 整个dataset的熵\n",
    "    bestInfoGainRatio = 0.0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataset]\n",
    "        uniqueVals = set(featList)\n",
    "        newEnt = 0.0\n",
    "        splitInfo = 0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataset = splitDataSet(dataset, i, value)  #每个唯一值对应的剩余feature的组成子集\n",
    "            prob = len(subDataset)/float(len(dataset))\n",
    "            newEnt += prob * calcEnt(subDataset)\n",
    "            splitInfo += -prob * log(prob, 2)\n",
    "        infoGain = baseEnt - newEnt\n",
    "        if (splitInfo == 0):\n",
    "            continue\n",
    "        infoGainRatio = infoGain/splitInfo\n",
    "        if (infoGainRatio > bestInfoGainRatio):\n",
    "            bestInfoGainRatio = infoGainRatio\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "\n",
    "# 划分数据，为下一层计算准备\n",
    "def splitDataSet(dataset, axis, value):\n",
    "    retDataset = list()\n",
    "    for featvec in dataset:\n",
    "        if featvec[axis] == value:\n",
    "            reduceFeatVec = featvec[:axis]\n",
    "            reduceFeatVec.extend(featvec[axis+1:])\n",
    "            retDataset.append(reduceFeatVec)\n",
    "\n",
    "    return retDataset\n",
    "\n",
    "# 构建DTree\n",
    "def createTree(dataset, labels):\n",
    "    classList = [example[-1] for example in dataset]\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        # classList所有元素都相等，即类别完全相同，停止划分\n",
    "        return classList[0]\n",
    "    if len(dataset[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    bestFeature = chooseBestFeature(dataset)\n",
    "    # 选择最大的gain ratio对应的feature\n",
    "    bestFeatLabel = labels[bestFeature]\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    del(labels[bestFeature])\n",
    "\n",
    "    featValues = [example[bestFeature] for example in dataset]\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataset, bestFeature, value), subLabels)\n",
    "\n",
    "    return myTree\n",
    "\n",
    "dataSet, labels = createDataSet()\n",
    "labels_tmp = labels[:]\n",
    "desicionTree = createTree(dataSet, labels_tmp)\n",
    "print(desicionTree)\n",
    "#treePlotter.createPlot(desicionTree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
