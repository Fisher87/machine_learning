{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习各优化算法总结\n",
    "\n",
    "[Reference](http://blog.csdn.net/qsczse943062710/article/details/76763739)\n",
    "\n",
    "> 优化算法包括：梯度下降法（Gradient Descent），共轭梯度法（Conjugate Gradient），Momentum算法及其变体，牛顿法和拟牛顿法（包括L-BFGS），AdaGrad，Adadelta，RMSprop，Adam及其变体，Nadam。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降算法(Gradient Descent)\n",
    "> 梯度下降法的核心思想是：它通过每次在当前梯度方向（最陡峭的方向）向前“迈”一步，来逐渐逼近函数的最小值；\n",
    "\n",
    "假设损失函数为$$ L(\\theta)$$在第n次迭代中，参数 $$ \\theta_n=\\theta_{n−1}+\\Delta\\theta$$\n",
    "我们将损失函数在$\\theta_{n−1}$处进行**一阶泰勒展开**：\n",
    "$$L(\\theta_n)=L(\\theta_{n−1}+\\Delta\\theta) \\approx L(\\theta_{n−1})+L^{'}(\\theta_{n−1})\\Delta\\theta$$\n",
    "为了使$L(\\theta_{n})<L(\\theta_{n-1})$，可以取$\\Delta\\theta=\\alpha L^{'}(\\theta_{n-1})$，带入得:\n",
    "$$L(\\theta_n)=L(\\theta_{n−1}+\\Delta\\theta)=L(\\theta_{n−1})-\\alpha L^{'}(\\theta_{n−1})^2$$\n",
    "式中总是满足$L(\\theta_{n})<L(\\theta_{n-1})$\n",
    "所以我们的梯度下降迭代公式为:\n",
    "$$\\theta_{n}:= \\theta_{n-1}-\\alpha L^{'}(\\theta_{n-1})$$\n",
    "梯度下降法根据每次求解损失函数时带入的样本数可以分为：*全量梯度下降*(计算所有样本的损失)、*批量梯度下降*(每次计算一个batch样本的损失)、*随机梯度下降*(每次随机选取一个样本计算损失)\n",
    "\n",
    "**现在所说的SGD（随机梯度下降）多指Mini-batch-Gradient-Descent（批量梯度下降）**\n",
    "\n",
    "---\n",
    "\n",
    "### 随机梯度下降算法(SGD)\n",
    "+ 优点：操作简单，计算量小，在损失函数是凸函数的情况下能够保证收敛到一个较好的全局最优解；\n",
    "+ 缺点：\n",
    "    1. $\\alpha$是个定值（在最原始的版本），它的选取直接决定了解的好坏，过小会导致收敛太慢，过大会导致震荡而无法收敛到最优解\n",
    "    2. 对于非凸问题，只能收敛到局部最优，并且没有任何摆脱局部最优的能力（一旦梯度为0就不会再有任何变化）。\n",
    "\n",
    "---\n",
    "\n",
    "### Momentum\n",
    "+ **思想：**在**SGD**中，每次的步长一致，并且方向都是当前梯度的方向，这会收敛的不稳定性：无论在什么位置，总是以相同的“步子”向前迈。Momentum的思想就是模拟物体运动的惯性：当我们跑步时转弯，我们最终的前进方向是由我们之前的方向和转弯的方向共同决定的。**Momentum**在每次更新时，保留一部分上次的更新方向：\n",
    "$$\\Delta\\theta_{n} = \\rho\\Delta\\theta_{n-1}+g_{n-1}$$\n",
    "$$\\theta_{n}:= \\theta_{n-1}-\\alpha \\Delta\\theta_{n}$$\n",
    "这里$\\rho$值决定了保留多少上次更新方向的信息，值为0~1，初始时可以取0.5，随着迭代逐渐增大；$\\alpha$为学习率，同SGD。\n",
    "+ **优点：**一定程度上缓解了SGD收敛不稳定的问题，并且有一定的摆脱局部最优的能力（当前梯度为0时，仍可能按照上次迭代的方向冲出局部最优点），直观上理解，它可以让每次迭代的“掉头方向不是那个大”。\n",
    "+ **缺点：**这里又多了另外一个超参数$\\rho$需要我们设置，它的选取同样会影响到结果。\n",
    "\n",
    "---\n",
    "\n",
    "### 自适应梯度法(Adagrad)\n",
    "+ **思想：**自适应梯度法，即 adaptive gradient；它通过记录每次迭代过程中的前进方向和距离，从而使得针对不同的问题有一套自适应调整学习率的方法：\n",
    "$$\\Delta\\theta_{n}= \\frac {1} {\\sqrt{\\sum_{i=1}^{n-1} g_{i}+ \\epsilon}} g_{n-1}$$\n",
    "$$\\theta_{n}:= \\theta_{n-1}-\\alpha \\Delta\\theta_{n}$$\n",
    "> 可以看到，随着迭代的增加，我们的学习率是在逐渐变小的，这在“直观上”是正确的：当我们越接近最优解时，函数的“坡度”会越平缓，我们也必须走的更慢来保证不会穿过最优解。这个变小的幅度只跟当前问题的函数梯度有关，ϵ是为了防止0除，一般取1e-7。\n",
    "\n",
    "+ **优点：** 解决了SGD中学习率不能自适应调整的问题\n",
    "+ **缺点：**需要手动设置$\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 牛顿法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "牛顿法不仅使用了一阶导，而且还利用了**二阶导来更新参数值**，公式如下所示：\n",
    "$$\\theta_{n}:= \\theta_{n-1} - \\alpha \\frac{L^{'}_{n-1}}{L^{''}_{n-1}}$$\n",
    "\n",
    "将损失函数在$\\theta_{n-1}$处进行**二阶泰勒展开**：\n",
    "\n",
    "$$\n",
    "L(\\theta_{n})  = L(\\theta_{n-1} + \\Delta\\theta) \\approx L(\\theta_{n-1}) + \\Biggl( L^{'}(\\theta_{n-1})\\Delta\\theta + \\frac{L^{''}(\\theta_{n-1})\\Delta\\theta^{2}}{2} \\Biggr)\n",
    "$$\n",
    "\n",
    "要使得$L(\\theta_{n})<L(\\theta_{n-1})$，极小化$K=L^{'}(\\theta_{n-1})\\Delta\\theta + \\frac{L^{''}(\\theta_{n-1})\\Delta\\theta^{2}}{2}$，\n",
    "对$K$求导，并赋值为0，可得到：\n",
    "$$\\Delta\\theta= - \\frac{L^{'}_{n-1}}{L^{''}_{n-1}}$$\n",
    "\n",
    "上式即为牛顿法的迭代公式，扩展到高维数据，二阶导变为**[Hession](https://zh.wikipedia.org/wiki/%E6%B5%B7%E6%A3%AE%E7%9F%A9%E9%98%B5)**矩阵，上式变为：\n",
    "$$\\Delta\\theta = -H^{-1}L^{'}_{n-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
