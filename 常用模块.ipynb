{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `random embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_embedding(vocab, embedding_dim):\n",
    "    embedding_mat = np.random.uniform(-0.25, 0.25, (len(vocab), embedding_dim))\n",
    "    embedding_mat = np.float32(embedding_mat)\n",
    "    return embedding_mat\n",
    "\n",
    "ori_embedding = random_embedding(vocab, embedding_dim)\n",
    "\n",
    "def embedding(word_ids):\n",
    "    _word_embedding = tf.Variable(ori_embedding, \n",
    "                                  dtype=tf.float32,\n",
    "                                  trainable = True,\n",
    "                                  name=\"_word_embedding\")\n",
    "    word_embedding = tf.nn.embedding_lookup(params=_word_embedding,\n",
    "                                            ids=word_ids, \n",
    "                                            name=\"word_embedding\")\n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_op():\n",
    "    if CRF:\n",
    "        log_likelihood, self.transition_params = crf_log_likelihood(inputs=self.logits, \n",
    "                                                                   tag_indices=self.labels,\n",
    "                                                                   sequence_lengths=self.sequence_lengths)\n",
    "        self.loss = -tf.reduce_mean(log_likelihood)\n",
    "        \n",
    "    else:\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, \n",
    "                                                                labels=self.labels)\n",
    "        mask = tf.sequence_mask(self.sequece_length)\n",
    "        losses = tf.boolean_mask(losses, mask)\n",
    "        self.loss = tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(inputs,queries=None, keys=None, type=None):\n",
    "    padding_num = -2**32 + 1\n",
    "    if type in (\"k\", \"key\", \"keys\"):\n",
    "        #generate masks\n",
    "        masks = tf.sign(tf.reduce_sum(tf.abs(keys), axis=-1))  # (N, T_k, d) ==> (N, T_K)\n",
    "        masks = tf.expand_dims(masks, 1)                       # (N, T_K) ==> (N, 1, T_K)\n",
    "        masks = tf.tile(masks, [1, tf.shape(queries)[1], 1])   # (N, 1, T_K) ==> (N, T_q, T_K)\n",
    "        \n",
    "        paddings = tf.ones_like(inputs) * padding_num\n",
    "       \n",
    "        outputs = tf.where(tf.equal(masks, 0), paddings, inputs)\n",
    "    return (outputs, masks, paddings)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 4.0000000e+00 -4.2949673e+09]\n",
      "  [ 8.0000000e+00 -4.2949673e+09]\n",
      "  [ 0.0000000e+00 -4.2949673e+09]]]\n",
      "[[[1. 0.]\n",
      "  [1. 0.]\n",
      "  [1. 0.]]]\n",
      "[[[-4.2949673e+09 -4.2949673e+09]\n",
      "  [-4.2949673e+09 -4.2949673e+09]\n",
      "  [-4.2949673e+09 -4.2949673e+09]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "queries = tf.constant([[[1.],\n",
    "                        [2.],\n",
    "                        [0.]]], tf.float32)\n",
    "\n",
    "keys = tf.constant([[[4.],\n",
    "                     [0.]]], tf.float32)\n",
    "inputs = tf.constant([[[4., 0.],\n",
    "                       [8., 0.],\n",
    "                       [0., 0.]]], tf.float32)\n",
    "outputs = mask(inputs, queries, keys, \"key\")\n",
    "with tf.Session() as sess:\n",
    "    _ = sess.run(outputs)\n",
    "    \n",
    "    (output, mask, padding) = _\n",
    "    print output\n",
    "    print mask\n",
    "    print padding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"generate a batch iterator for a dataset\"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffle_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "            \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
