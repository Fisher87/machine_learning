{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTR预估\n",
    "\n",
    "> 常用方案有：LR、GBDT+LR、FM+LR、FFM+LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 热门推荐\n",
    "+ [广告点击率模型中，LR, GBDT+LR, FM, DNN等模型的优点和缺点？实际效果如何?](https://www.zhihu.com/question/62109451)\n",
    "\n",
    "+ [我用过的CTR预估模型](https://zhuanlan.zhihu.com/p/27957325)\n",
    "\n",
    "+ [CTR预估系列一览表](https://zhuanlan.zhihu.com/p/31589565?refer=c_124379826)\n",
    "\n",
    "+ [常见计算广告点击率预估算法总结](https://zhuanlan.zhihu.com/p/29053940?group_id=916424345212342272)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from sklearn.externals import joblib  \n",
    "\n",
    "# show predict info\n",
    "def model_metrics(clf, x_train, x_test, y_train, y_test):\n",
    "    from sklearn import metrics\n",
    "    prepro = clf.predict_proba(x_train)[:,1]\n",
    "    y_predict = clf.predict(x_train)\n",
    "    \n",
    "    logging.info('')\n",
    "    logging.info('ACC train: %s' %  metrics.accuracy_score(y_train, y_predict))\n",
    "    logging.info('rcall train: %s' % metrics.recall_score(y_train, y_predict))\n",
    "    logging.info('AUC train:: %s' %  metrics.roc_auc_score(y_train, prepro))\n",
    "    logging.info('%s %s' % (len(y_train), sum(y_train)/ float(len(y_train))))\n",
    "\n",
    "    prepro = clf.predict_proba(x_test)[:,1]\n",
    "    y_predict = clf.predict(x_test)\n",
    "\n",
    "    logging.info('')\n",
    "    logging.info('ACC test: %s' % metrics.accuracy_score(y_test, y_predict))\n",
    "    logging.info('rcall test %s' % metrics.recall_score(y_test, y_predict))\n",
    "    logging.info('AUC: %s' %  metrics.roc_auc_score(y_test, prepro))\n",
    "\n",
    "    logging.info( '%s %s' % (len(y_test), sum(y_test)/ float(len(y_test))))\n",
    "    \n",
    "# save model\n",
    "def save(model, name='ctr.pkl'):\n",
    "    joblib.dump(model, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LR算法介绍](http://blog.csdn.net/pakko/article/details/37878837)\n",
    "\n",
    "> LR模型框架\n",
    "\n",
    "```python\n",
    "class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "```\n",
    "\n",
    "#### 参数解析：\n",
    "+ **penalty:** 正则化选择参数（惩罚项的种类）；\n",
    "penalty参数的选择会影响我们损失函数优化算法的选择。即参数solver的选择，如果是L2正则化，那么4种可选的算法{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}都可以选择。但是如果penalty是L1正则化的话，就只能选择‘liblinear’了。这是因为*L1正则化的损失函数不是连续可导的，而{‘newton-cg’, ‘lbfgs’,‘sag’}这三种优化算法时都需要损失函数的一阶或者二阶连续导数*。而‘liblinear’并没有这个依赖。\n",
    "+ **C:**  C为正则化系数λ的倒数，通常默认为1，值越小正则化越强；\n",
    "+ **fit_intercept:** 是否存在截距，默认存在\n",
    "+ **class_weight:** 类型权重参数\n",
    "+ **solver:** 优化算法选择参数， {newton-cg，lbfgs，liblinear，sag}，默认`liblinear`\n",
    "    1. liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。\n",
    "    2. lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。\n",
    "    3. newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。\n",
    "    4. sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候\n",
    " ```\n",
    " newton-cg, lbfgs和sag这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的L1正则化，只能用于L2正则化。而liblinear通吃L1正则化和L2正则化。 同时，sag每次仅仅使用了部分样本进行梯度迭代，所以当样本量少的时候不要选择它，而如果样本量非常大，比如大于10万，sag是第一选择。但是sag不能用于L1正则化，所以当你有大量的样本，又需要L1正则化的话就要自己做取舍了。要么通过对样本采样来降低样本量，要么回到L2正则化。\n",
    "```\n",
    "+ **max_iter:** 最大迭代次数 \n",
    "+ **multi_class:** 分类方式选择参数\n",
    "\n",
    "#### FQA\n",
    "+ **为什么LR模型可以用来计算CTR？**\n",
    "\n",
    "在CTR问题中，可以把被点击的样本看成正例， 未点击的样本看成负例，这样就变成了一个二分类问题。样本的ctr实际上就是样本为正例的概率，LR模型虽然是分类模型，但同时可以输出样本为正例的概率，因此可以用来解决该问题。另外LR相比于其他模型有求解简单、可解释性强的优点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 示例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load package\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# load data\n",
    "datasets = np.read_csv()\n",
    "train, test = train_test_split(datasets, test_size=.2, random_state=0)\n",
    "X_train = train[:, 2:]\n",
    "y_train = train[:, 0]\n",
    "\n",
    "X_test  = test[:, 2:]\n",
    "y_test  = test[:, 0]\n",
    "\n",
    "# build LR model\n",
    "clf = LogisticRegression(\n",
    "                verbose=False, \n",
    "                max_iter=200, \n",
    "                C=0.16, \n",
    "                penalty='l2',\n",
    "                solver='lbfgs',\n",
    "                n_jobs=-1,\n",
    "                class_weight = 'balanced')\n",
    "\n",
    "# train model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# show model predict info\n",
    "model_metrics(clf, X_train, X_test, y_train, y_test)\n",
    "save(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT + LR 组合方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[GBDT算法介绍](https://cloud.tencent.com/developer/article/1005611)\n",
    "\n",
    "> GBDT模型框架\n",
    "\n",
    "``` python\n",
    "class sklearn.ensemble.GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "```\n",
    "#### 参数解析：\n",
    "##### 框架参数\n",
    "+ **n_estimators:** 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑；\n",
    "+ **learning_rate:** 即每个弱学习器的权重缩减系数νν，也称作步长，在原理篇的正则化章节我们也讲到了，加上了正则化项，我们的强学习器的迭代公式为f<sub>k</sub>(x)=f<sub>k-1</sub>(x)+vh<sub>k</sub>(x)。v的取值范围为0<v≤1。对于同样的训练集拟合效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的v开始调参，默认是1；\n",
    "+ **subsample:** 即我们在原理篇的正则化章节讲到的子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样；\n",
    "+ **loss:** 即我们GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样的；\n",
    "```text\n",
    "   **对于分类模型**，有对数似然损失函数\"deviance\"和指数损失函数\"exponential\"两者输入选择。默认是对数似然损失函数\"deviance\"。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的\"deviance\"。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。\n",
    "   **对于回归模型**，有均方差\"ls\", 绝对损失\"lad\", Huber损失\"huber\"和分位数损失“quantile”。默认是均方差\"ls\"。一般来说，如果数据的噪音点不多，用默认的均方差\"ls\"比较好。如果是噪音点较多，则推荐用抗噪音的损失函数\"huber\"。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。\n",
    "``` \n",
    "+ **alpha：**这个参数只有GradientBoostingRegressor有，当我们使用Huber损失\"huber\"和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。\n",
    "\n",
    "##### 学习器参数\n",
    "> 由于GBDT使用了CART回归决策树，因此它的参数基本来源于决策树类，也就是说，和DecisionTreeClassifier和DecisionTreeRegressor的参数基本类似\n",
    "\n",
    "+ **max_features:** 划分时考虑的最大特征数，一般来说，如果样本特征数不多，比如小于50，我们用默认的\"None\"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间；\n",
    "+ **max_depth:** 决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间；\n",
    "+ **min_samples_split:** 内部节点再划分所需最小样本数，这个值限制了子树继续划分的条件，如果某节点的样本数少于`min_sample_split`， 则不会继续在尝试选择最优特征来进行划分，默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值；\n",
    "+ **min_samples_leaf:** 叶子节点最少样本数，这个值限制了叶子节点最少的样本数，如果某个叶子节点数目少于样本数，则会和兄弟节点一起被剪枝。默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值；\n",
    "+ **min_weight_fraction_leaf:** 叶子节点最小的样本权重和，这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了；\n",
    "+ **max_leaf_nodes:** 最大叶子节点数，通过限制最大叶子节点数，可以防止过拟合，默认是\"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到；\n",
    "+ **min_impurity_split:** 节点划分最小不纯度， 通过限制最大叶子节点数，可以防止过拟合，默认是\"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方案思想\n",
    "用已有特征训练GBDT模型，然后利用GBDT模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值0/1的，向量的每个元素对应于GBDT模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为1，而这棵树的其他叶子结点对应的元素值为0。新特征向量的长度等于GBDT模型里所有树包含的叶子结点数之和。\n",
    "例如：\n",
    "![imgage](./docs/gbdt_lr.png)\n",
    "\n",
    "##### FQA\n",
    "+ **为什么建树采用ensemble决策树？**\n",
    "\n",
    "一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT每棵树都在学习前面棵树尚存的不足，迭代多少次就会生成多少颗树。按paper以及Kaggle竞赛中的GBDT+LR融合方式，多棵树正好满足LR每条训练样本可以通过GBDT映射成多个特征的需求。\n",
    "+ **为什么建树采用GBDT而非RF？**\n",
    "\n",
    "RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 示例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin python\n",
    "#-*- coding:utf-8 -*-\n",
    "from scipy.sparse.construct import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets.svmlight_format import load_svmlight_file\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "from sklearn.preprocessing.data import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "def gbdt_lr_train(libsvmFileName):\n",
    "\n",
    "    # load样本数据\n",
    "    X_all, y_all = load_svmlight_file(libsvmFileName)\n",
    "\n",
    "    # 训练/测试数据分割\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.3, random_state = 42)\n",
    "\n",
    "    # 定义GBDT模型\n",
    "    gbdt = GradientBoostingClassifier(\n",
    "                n_estimators=40, \n",
    "                max_depth=3, \n",
    "                verbose=0,\n",
    "                max_features=0.5)\n",
    "\n",
    "    # 训练学习\n",
    "    gbdt.fit(X_train, y_train)\n",
    "\n",
    "    # 预测及AUC评测\n",
    "    y_pred_gbdt = gbdt.predict_proba(X_test.toarray())[:, 1]\n",
    "    gbdt_auc = roc_auc_score(y_test, y_pred_gbdt)\n",
    "    print('gbdt auc: %.5f' % gbdt_auc)\n",
    "\n",
    "    # lr对原始特征样本模型训练\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)    # 预测及AUC评测\n",
    "    y_pred_test = lr.predict_proba(X_test)[:, 1]\n",
    "    lr_test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "    print('基于原有特征的LR AUC: %.5f' % lr_test_auc)\n",
    "\n",
    "    # GBDT编码原有特征\n",
    "    X_train_leaves = gbdt.apply(X_train)[:,:,0]\n",
    "    X_test_leaves = gbdt.apply(X_test)[:,:,0]\n",
    "\n",
    "    # 对所有特征进行ont-hot编码\n",
    "    (train_rows, cols) = X_train_leaves.shape\n",
    "\n",
    "    gbdtenc = OneHotEncoder()\n",
    "    X_trans = gbdtenc.fit_transform(np.concatenate((X_train_leaves, X_test_leaves), axis=0))\n",
    "\n",
    "    # 定义LR模型\n",
    "    lr = LogisticRegression()\n",
    "    # lr对gbdt特征编码后的样本模型训练\n",
    "    lr.fit(X_trans[:train_rows, :], y_train)\n",
    "    # 预测及AUC评测\n",
    "    y_pred_gbdtlr1 = lr.predict_proba(X_trans[train_rows:, :])[:, 1]\n",
    "    gbdt_lr_auc1 = roc_auc_score(y_test, y_pred_gbdtlr1)\n",
    "    print('基于GBDT特征编码后的LR AUC: %.5f' % gbdt_lr_auc1)\n",
    "\n",
    "    # 定义LR模型\n",
    "    lr = LogisticRegression(n_jobs=-1)\n",
    "    # 组合特征\n",
    "    X_train_ext = hstack([X_trans[:train_rows, :], X_train])\n",
    "    X_test_ext = hstack([X_trans[train_rows:, :], X_test])\n",
    "\n",
    "    print(X_train_ext.shape)\n",
    "    # lr对组合特征的样本模型训练\n",
    "    lr.fit(X_train_ext, y_train)\n",
    "\n",
    "    # 预测及AUC评测\n",
    "    y_pred_gbdtlr2 = lr.predict_proba(X_test_ext)[:, 1]\n",
    "    gbdt_lr_auc2 = roc_auc_score(y_test, y_pred_gbdtlr2)\n",
    "    print('基于组合特征的LR AUC: %.5f' % gbdt_lr_auc2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gbdt_lr_train('data/sample_libsvm_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST + LR 组合方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin python\n",
    "#-*- coding:utf-8 -*-\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "\n",
    "def xgb_feature_encode(libsvmFileNameInitial):\n",
    "\n",
    "    # load样本数据\n",
    "    X_all, y_all = load_svmlight_file(libsvmFileNameInitial)\n",
    "\n",
    "    # 训练/测试数据分割\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.3, random_state = 42)\n",
    "\n",
    "    # 定义模型\n",
    "    xgboost = xgb.XGBClassifier(nthread=4, learning_rate=0.08,\n",
    "                            n_estimators=50, max_depth=5, gamma=0, subsample=0.9, colsample_bytree=0.5)\n",
    "    # 训练学习\n",
    "    xgboost.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # 预测及AUC评测\n",
    "    y_pred_test = xgboost.predict_proba(X_test)[:, 1]\n",
    "    xgb_test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "    print('xgboost test auc: %.5f' % xgb_test_auc)\n",
    "\n",
    "    # xgboost编码原有特征\n",
    "    X_train_leaves = xgboost.apply(X_train)\n",
    "    X_test_leaves = xgboost.apply(X_test)\n",
    "    # 训练样本个数\n",
    "    train_rows = X_train_leaves.shape[0]\n",
    "    # 合并编码后的训练数据和测试数据\n",
    "    X_leaves = np.concatenate((X_train_leaves, X_test_leaves), axis=0)\n",
    "    X_leaves = X_leaves.astype(np.int32)\n",
    "\n",
    "    (rows, cols) = X_leaves.shape\n",
    "\n",
    "    # 记录每棵树的编码区间\n",
    "    cum_count = np.zeros((1, cols), dtype=np.int32)\n",
    "\n",
    "    for j in range(cols):\n",
    "        if j == 0:\n",
    "            cum_count[0][j] = len(np.unique(X_leaves[:, j]))\n",
    "        else:\n",
    "            cum_count[0][j] = len(np.unique(X_leaves[:, j])) + cum_count[0][j-1]\n",
    "\n",
    "    print('Transform features genenrated by xgboost...')\n",
    "    # 对所有特征进行ont-hot编码\n",
    "    for j in range(cols):\n",
    "        keyMapDict = {}\n",
    "        if j == 0:\n",
    "            initial_index = 1\n",
    "        else:\n",
    "            initial_index = cum_count[0][j-1]+1\n",
    "        for i in range(rows):\n",
    "            if X_leaves[i, j] not in keyMapDict:\n",
    "                keyMapDict[X_leaves[i, j]] = initial_index\n",
    "                X_leaves[i, j] = initial_index\n",
    "                initial_index = initial_index + 1\n",
    "            else:\n",
    "                X_leaves[i, j] = keyMapDict[X_leaves[i, j]]\n",
    "\n",
    "    # 基于编码后的特征，将特征处理为libsvm格式且写入文件\n",
    "    print('Write xgboost learned features to file ...')\n",
    "    xgbFeatureLibsvm = open('data/xgb_feature_libsvm', 'w')\n",
    "    for i in range(rows):\n",
    "        if i < train_rows:\n",
    "            xgbFeatureLibsvm.write(str(y_train[i]))\n",
    "        else:\n",
    "            xgbFeatureLibsvm.write(str(y_test[i-train_rows]))\n",
    "        for j in range(cols):\n",
    "            xgbFeatureLibsvm.write(' '+str(X_leaves[i, j])+':1.0')\n",
    "        xgbFeatureLibsvm.write('\\n')\n",
    "    xgbFeatureLibsvm.close()\n",
    "\n",
    "\n",
    "def xgboost_lr_train(xgbfeaturefile, origin_libsvm_file):\n",
    "\n",
    "    # load xgboost特征编码后的样本数据\n",
    "    X_xg_all, y_xg_all = load_svmlight_file(xgbfeaturefile)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_xg_all, y_xg_all, test_size = 0.3, random_state = 42)\n",
    "\n",
    "    # load 原始样本数据\n",
    "    X_all, y_all = load_svmlight_file(origin_libsvm_file)\n",
    "    X_train_origin, X_test_origin, y_train_origin, y_test_origin = train_test_split(X_all, y_all, test_size = 0.3, random_state = 42)\n",
    "\n",
    "\n",
    "    # lr对原始特征样本模型训练\n",
    "    lr = LogisticRegression(n_jobs=-1, C=0.1, penalty='l1')\n",
    "    lr.fit(X_train_origin, y_train_origin)\n",
    "    joblib.dump(lr, 'model/lr_orgin.m')\n",
    "    # 预测及AUC评测\n",
    "    y_pred_test = lr.predict_proba(X_test_origin)[:, 1]\n",
    "    lr_test_auc = roc_auc_score(y_test_origin, y_pred_test)\n",
    "    print('基于原有特征的LR AUC: %.5f' % lr_test_auc)\n",
    "\n",
    "    # lr对load xgboost特征编码后的样本模型训练\n",
    "    lr = LogisticRegression(n_jobs=-1, C=0.1, penalty='l1')\n",
    "    lr.fit(X_train, y_train)\n",
    "    joblib.dump(lr, 'model/lr_xgb.m')\n",
    "    # 预测及AUC评测\n",
    "    y_pred_test = lr.predict_proba(X_test)[:, 1]\n",
    "    lr_test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "    print('基于Xgboost特征编码后的LR AUC: %.5f' % lr_test_auc)\n",
    "\n",
    "    # 基于原始特征组合xgboost编码后的特征\n",
    "    X_train_ext = hstack([X_train_origin, X_train])\n",
    "    del(X_train)\n",
    "    del(X_train_origin)\n",
    "    X_test_ext = hstack([X_test_origin, X_test])\n",
    "    del(X_test)\n",
    "    del(X_test_origin)\n",
    "\n",
    "    # lr对组合后的新特征的样本进行模型训练\n",
    "    lr = LogisticRegression(n_jobs=-1, C=0.1, penalty='l1')\n",
    "    lr.fit(X_train_ext, y_train)\n",
    "    joblib.dump(lr, 'model/lr_ext.m')\n",
    "    # 预测及AUC评测\n",
    "    y_pred_test = lr.predict_proba(X_test_ext)[:, 1]\n",
    "    lr_test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "    print('基于组合特征的LR AUC: %.5f' % lr_test_auc)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    xgb_feature_encode(\"data/sample_libsvm_data.txt\")\n",
    "    xgboost_lr_train(\"data/xgb_feature_libsvm\",\"data/sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FM+LR 组合方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FM与FFM算法介绍](http://blog.csdn.net/jediael_lu/article/details/77772565)\n",
    "\n",
    "[深入浅出FM](http://www.52caml.com/head_first_ml/ml-chapter9-factorization-family/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFM+LR 组合方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FM与FFM算法介绍](http://blog.csdn.net/jediael_lu/article/details/77772565)\n",
    "\n",
    "[深入浅出FFM](https://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
